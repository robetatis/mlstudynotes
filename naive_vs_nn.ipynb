{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on to single-cell predictions, let's look at an example that illustrates this situation: we'll generate synthetic autoregressive series of different lengths and train larger networks than we've used so far, and then compare them to the na√Øve model $x(t) = x(t-1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def make_autoregressive_series(n):\n",
    "\n",
    "    sigma = 20. # variance of noise term\n",
    "    delta = 0. # drift\n",
    "    phi1 = 0.75 # 1st-order autoregression coefficient\n",
    "    w = np.random.normal(loc=0, scale=2, size=n) # noise vector\n",
    "    t = np.arange(n) # time vector\n",
    "    x = np.zeros(n, float) # initialize series\n",
    "    T1, T2, T3 = 100000, 5000, 500 # periods of seasonal components\n",
    "    s1, s2, s3 = np.sin(2.*np.pi/T1*t), np.sin(2.*np.pi/T2*t), np.sin(2*np.pi/T3*t)\n",
    "    x[0] = 0.0 # first value of series\n",
    "\n",
    "    # build series\n",
    "    for i in range(1, n, 1):\n",
    "        x[i] = delta + phi1*x[i-1] + + s1[i] + s2[i] + s3[i] + w[i]\n",
    "    \n",
    "    x = pd.DataFrame(x, index=t, columns=['x'])\n",
    "\n",
    "    return x\n",
    "\n",
    "x = make_autoregressive_series(n=300000)\n",
    "\n",
    "big_datasets = split_and_standarize_data(x, f_train=0.6, f_val=0.2)\n",
    "\n",
    "CONV_WIDTH = 50\n",
    "LABEL_WIDTH = 1\n",
    "INPUT_WIDTH = LABEL_WIDTH + (CONV_WIDTH - 1)\n",
    "\n",
    "conv_bigdata = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=30, kernel_size=(CONV_WIDTH // 2,)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(30),\n",
    "    tf.keras.layers.Dense(units=1),\n",
    "    tf.keras.layers.Reshape((1, 1))\n",
    "])\n",
    "\n",
    "w_conv_bigdata = WindowGenerator(\n",
    "    input_width=INPUT_WIDTH, \n",
    "    label_width=LABEL_WIDTH, \n",
    "    offset=1, \n",
    "    train_df=big_datasets['train_df'],\n",
    "    val_df=big_datasets['val_df'],\n",
    "    test_df=big_datasets['test_df'],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_columns=['x'])\n",
    "\n",
    "print(f'X shape = {w_conv_bigdata.example[0].shape}')\n",
    "print(f'y_pred shape = {conv_bigdata(w_conv_bigdata.example[0]).shape}')\n",
    "print(f'y_obs shape = {w_conv_bigdata.example[1].shape}')\n",
    "\n",
    "conv_bigdata(w_conv_bigdata.example[0]) # build model to get number of parameters\n",
    "\n",
    "print(f'Estimating {conv_bigdata.count_params():,} parameters on {w_conv_bigdata.train_df.shape[0]:,} datapoints')\n",
    "\n",
    "history_conv_bigdata = compile_and_fit(conv_bigdata, w_conv_bigdata, max_epochs=MAX_EPOCHS, patience=20, verbose=1)\n",
    "\n",
    "ypred_conv_bigdata = make_ypred(conv_bigdata, w_conv_bigdata, big_datasets['train_mean']['x'], big_datasets['train_sd']['x'])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(history_conv_bigdata.history['loss'], label='train')\n",
    "ax.plot(history_conv_bigdata.history['val_loss'], label='val')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ypred['conv_bigdata'] = ypred_conv_bigdata\n",
    "err_rel['conv_bigdata'] = 100. * (ypred['conv_bigdata'] - big_datasets['test_df_original']['x']) / big_datasets['test_df_original']['x']\n",
    "err_abs['conv_bigdata'] = ypred['conv_bigdata'] - big_datasets['test_df_original']['x']\n",
    "\n",
    "# add ypred for naive big data baseline model\n",
    "ypred_naive_bigdata = big_datasets['test_df_original']['x'][:-1].values\n",
    "ytest_naive_bigdata = big_datasets['test_df_original']['x'][1:].values\n",
    "ypred['naive_bigdata'] = ypred_naive_bigdata\n",
    "err_rel['naive_bigdata'] = 100. * (ypred['naive_bigdata'] - ytest_naive_bigdata)/ytest_naive_bigdata\n",
    "err_abs['naive_bigdata'] = ypred['naive_bigdata'] - ytest_naive_bigdata\n",
    "\n",
    "bins = np.arange(-300, 305, 5)\n",
    "fig = plt.figure(figsize=(15, 4))\n",
    "gs = gridspec.GridSpec(2, 4)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[:, :3])\n",
    "ax1.plot(big_datasets['test_timestamps'], big_datasets['test_df_original']['x'], linewidth=1, color='black', label='y_test')\n",
    "ax1.plot(big_datasets['test_timestamps'], ypred_conv_bigdata, linewidth=1, color='red', alpha=0.5, label=f'conv1d')\n",
    "ax1.set_xlim(280000, 280400)\n",
    "ax1.legend(loc=1)\n",
    "ax2 = fig.add_subplot(gs[0, 3:])\n",
    "ax2.set_title('Naive big data')\n",
    "ax2.hist(err_rel['naive_bigdata'], bins=bins, density=True)\n",
    "ax3 = fig.add_subplot(gs[1, 3:])\n",
    "ax3.set_title('Conv1D big data')\n",
    "ax3.hist(err_rel['conv_bigdata'], bins=bins, density=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "perc_rel_errors_below_10['naive_bigdata'] = 100.*err_rel['naive_bigdata'][err_rel['naive_bigdata'] <= 10].size/err_rel['naive_bigdata'].size\n",
    "perc_rel_errors_below_10['conv_bigdata'] = 100.*err_rel['conv_bigdata'][err_rel['conv_bigdata'] <= 10].size/err_rel['conv_bigdata'].size\n",
    "\n",
    "print(perc_rel_errors_below_10['naive_bigdata'])\n",
    "print(perc_rel_errors_below_10['conv_bigdata'])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
